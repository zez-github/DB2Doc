"""
APIè·¯ç”±æ¨¡å—
"""

from flask import Blueprint, request, jsonify, Response, send_file
import os
import json
import queue
from datetime import datetime
import threading
from datetime import datetime
from pathlib import Path
import tkinter as tk
from tkinter import filedialog

from ..utils import connect_db, get_tables_and_views, get_columns_info, infer_chinese_meaning, generate_markdown, get_databases

api_bp = Blueprint('api', __name__, url_prefix='/api')

# å…¨å±€æ—¥å¿—é˜Ÿåˆ—
log_queue = queue.Queue()


def log_message(message):
    """è®°å½•æ—¥å¿—æ¶ˆæ¯"""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    log_queue.put(f"[{timestamp}] {message}")


@api_bp.route('/test_connection', methods=['POST'])
def test_connection():
    """æµ‹è¯•æ•°æ®åº“è¿æ¥"""
    try:
        data = request.get_json()
        host = data.get('host')
        user = data.get('user')
        password = data.get('password')
        port = int(data.get('port', 3306 if data.get('db_type', 'mysql') == 'mysql' else 1433))
        database = data.get('database')
        db_type = data.get('db_type', 'mysql')

        connection = connect_db(host, user, password, port, database, db_type)
        tables = get_tables_and_views(connection, database, db_type)
        connection.close()
        return jsonify({"success": True, "tables": tables, "db_type": db_type})
    except Exception as e:
        return jsonify({"success": False, "message": str(e)})


@api_bp.route('/get_tables', methods=['POST'])
def get_tables():
    """è·å–æ•°æ®åº“è¡¨åˆ—è¡¨"""
    try:
        data = request.get_json()
        host = data.get('host')
        user = data.get('user')
        password = data.get('password')
        port = int(data.get('port', 3306 if data.get('db_type', 'mysql') == 'mysql' else 1433))
        database = data.get('database')
        db_type = data.get('db_type', 'mysql')

        connection = connect_db(host, user, password, port, database, db_type)
        tables = get_tables_and_views(connection, database, db_type)
        connection.close()
        
        return jsonify({"success": True, "tables": tables})
    except Exception as e:
        return jsonify({"success": False, "message": str(e)})


@api_bp.route('/list_databases', methods=['POST'])
def list_databases():
    """è·å–æœåŠ¡å™¨ä¸Šçš„æ•°æ®åº“åˆ—è¡¨"""
    try:
        data = request.get_json()
        host = data.get('host')
        user = data.get('user')
        password = data.get('password')
        db_type = data.get('db_type', 'mysql')
        # æ ¹æ®æ•°æ®åº“ç±»å‹è®¾ç½®é»˜è®¤ç«¯å£
        port = int(data.get('port', 3306 if db_type == 'mysql' else 1433))

        databases = get_databases(host, user, password, port, db_type)
        return jsonify({"success": True, "databases": databases})
    except Exception as e:
        return jsonify({"success": False, "message": str(e)})


@api_bp.route('/parse_doc', methods=['POST'])
def parse_doc():
    """è§£ææ–‡æ¡£"""
    try:
        data = request.get_json()
        file_path = data.get('doc_path')
        if not file_path or not os.path.exists(file_path):
            return jsonify({"success": False, "message": "æ–‡æ¡£è·¯å¾„æ— æ•ˆæˆ–æ–‡ä»¶ä¸å­˜åœ¨"})
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        return jsonify({"success": True, "content": content})
    except Exception as e:
        return jsonify({"success": False, "message": str(e)})


@api_bp.route('/default_path', methods=['GET'])
def api_default_path():
    """è·å–é»˜è®¤è·¯å¾„"""
    def get_downloads_dir():
        # Windowsä¼˜å…ˆä½¿ç”¨ç”¨æˆ·ä¸»ç›®å½• + Downloadsï¼Œè‹¥ä¸å­˜åœ¨åˆ™é€€å›ä¸»ç›®å½•
        downloads = Path.home() / 'Downloads'
        return str(downloads if downloads.exists() else Path.home())

    try:
        return jsonify({"success": True, "path": get_downloads_dir()})
    except Exception as e:
        return jsonify({"success": False, "message": str(e)})


@api_bp.route('/select_directory', methods=['GET'])
def api_select_directory():
    """é€‰æ‹©ç›®å½•"""
    try:
        root = tk.Tk()
        root.withdraw()
        root.attributes('-topmost', True)
        path = filedialog.askdirectory(title='é€‰æ‹©ä¿å­˜æ–‡ä»¶å¤¹')
        root.destroy()
        if path:
            return jsonify({"success": True, "path": path})
        else:
            return jsonify({"success": False, "message": "ç”¨æˆ·å–æ¶ˆ"})
    except Exception as e:
        return jsonify({"success": False, "message": str(e)})


@api_bp.route('/generate_docs', methods=['POST'])
def generate_docs():
    """ç”Ÿæˆæ–‡æ¡£"""
    try:
        data = request.get_json()
        host = data.get('host')
        user = data.get('user')
        password = data.get('password')
        port = int(data.get('port', 3306 if data.get('db_type', 'mysql') == 'mysql' else 1433))
        database = data.get('database')
        db_type = data.get('db_type', 'mysql')
        selected_tables = data.get('tables', [])
        output_path = data.get('output_path', '')
        file_name = data.get('file_name', f'{database}_æ–‡æ¡£_{datetime.now().strftime("%Y%m%d_%H%M%S")}.md')

        incremental_mode = data.get('incremental_mode', False)
        existing_doc_path = data.get('existing_doc_path', '')
        existing_doc_content = data.get('existing_doc_content', '')
        existing_tables = data.get('existing_tables', [])
        db_description = data.get('db_description', '')
        
        # è°ƒè¯•æ—¥å¿—
        if incremental_mode:
            log_message(f"å¢é‡æ¨¡å¼è°ƒè¯• - existing_doc_path: {existing_doc_path}")
            log_message(f"å¢é‡æ¨¡å¼è°ƒè¯• - existing_doc_contenté•¿åº¦: {len(existing_doc_content) if existing_doc_content else 0}")
            log_message(f"å¢é‡æ¨¡å¼è°ƒè¯• - existing_tables: {existing_tables}")

        # æ¸…ç©ºæ—¥å¿—é˜Ÿåˆ—
        while not log_queue.empty():
            log_queue.get()

        def generate_in_background():
            try:
                connection = connect_db(host, user, password, port, database, db_type)

                if incremental_mode and existing_tables:
                    new_tables = [table for table in selected_tables if table not in existing_tables]
                    skipped_count = len(selected_tables) - len(new_tables)
                    if skipped_count > 0:
                        log_message(f"å¢é‡æ›´æ–°æ¨¡å¼ï¼šè·³è¿‡ {skipped_count} ä¸ªå·²å­˜åœ¨çš„è¡¨æ ¼")
                    selected_tables_final = new_tables
                else:
                    selected_tables_final = selected_tables

                # ç¡®å®šè¾“å‡ºæ–‡ä»¶è·¯å¾„
                if incremental_mode and existing_doc_path:
                    # å¢é‡æ›´æ–°æ¨¡å¼ï¼šç”Ÿæˆå¸¦æœ‰å¢é‡æ ‡è¯†çš„æ–°æ–‡ä»¶å
                    base_name = os.path.splitext(existing_doc_path)[0]  # å»æ‰æ‰©å±•å
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    incremental_file_name = f"{base_name}_å¢é‡æ›´æ–°_{timestamp}.md"
                    
                    if output_path:
                        user_output_path = output_path
                        if not os.path.isabs(user_output_path):
                            user_output_path = os.path.abspath(user_output_path)
                        if not os.path.exists(user_output_path):
                            os.makedirs(user_output_path, exist_ok=True)
                            log_message(f"åˆ›å»ºè¾“å‡ºç›®å½•: {user_output_path}")
                        output_file_path = os.path.join(user_output_path, incremental_file_name)
                        log_message(f"å¢é‡æ›´æ–°æ¨¡å¼ï¼šå°†æ›´æ–°æ–‡æ¡£ä¿å­˜åˆ°: {output_file_path}")
                    else:
                        default_output_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', '..', 'data', 'output')
                        if not os.path.exists(default_output_dir):
                            os.makedirs(default_output_dir, exist_ok=True)
                            log_message(f"åˆ›å»ºé»˜è®¤è¾“å‡ºç›®å½•: {default_output_dir}")
                        output_file_path = os.path.join(default_output_dir, incremental_file_name)
                        log_message(f"å¢é‡æ›´æ–°æ¨¡å¼ï¼šå°†æ›´æ–°æ–‡æ¡£ä¿å­˜åˆ°é»˜è®¤è·¯å¾„: {output_file_path}")
                    output_file = open(output_file_path, 'w', encoding='utf-8')
                else:
                    # æ™®é€šæ¨¡å¼ï¼šä½¿ç”¨ç”Ÿæˆçš„æ–‡ä»¶å
                    if output_path:
                        user_output_path = output_path
                        if not os.path.isabs(user_output_path):
                            user_output_path = os.path.abspath(user_output_path)
                        if not os.path.exists(user_output_path):
                            os.makedirs(user_output_path, exist_ok=True)
                            log_message(f"åˆ›å»ºè¾“å‡ºç›®å½•: {user_output_path}")
                        output_file_path = os.path.join(user_output_path, file_name)
                        output_file = open(output_file_path, 'w', encoding='utf-8')
                        log_message(f"æ–‡æ¡£å°†ä¿å­˜åˆ°: {output_file_path}")
                    else:
                        default_output_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', '..', 'data', 'output')
                        if not os.path.exists(default_output_dir):
                            os.makedirs(default_output_dir, exist_ok=True)
                            log_message(f"åˆ›å»ºé»˜è®¤è¾“å‡ºç›®å½•: {default_output_dir}")
                        output_file_path = os.path.join(default_output_dir, file_name)
                        output_file = open(output_file_path, 'w', encoding='utf-8')
                        log_message(f"ä½¿ç”¨é»˜è®¤è·¯å¾„ä¿å­˜æ–‡æ¡£: {output_file_path}")

                # åœ¨å¢é‡æ›´æ–°æ¨¡å¼ä¸‹ï¼Œå…ˆå†™å…¥ç°æœ‰æ–‡æ¡£å†…å®¹
                if incremental_mode:
                    log_message(f"å¢é‡æ›´æ–°æ¨¡å¼ï¼šæ£€æŸ¥ç°æœ‰æ–‡æ¡£å†…å®¹...")
                    log_message(f"existing_doc_contentæ˜¯å¦å­˜åœ¨: {existing_doc_content is not None}")
                    log_message(f"existing_doc_contenté•¿åº¦: {len(existing_doc_content) if existing_doc_content else 0}")
                    
                    if existing_doc_content:
                        log_message(f"å¢é‡æ›´æ–°æ¨¡å¼ï¼šå¼€å§‹å†™å…¥ç°æœ‰æ–‡æ¡£å†…å®¹...")
                        output_file.write(existing_doc_content)
                        # ç¡®ä¿ç°æœ‰å†…å®¹åæœ‰æ¢è¡Œç¬¦
                        if not existing_doc_content.endswith('\n'):
                            output_file.write('\n')
                        
                        # æ·»åŠ å¢é‡æ›´æ–°åˆ†éš”æ ‡è¯†
                        update_timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                        separator = f"\n\n<!-- ==================== å¢é‡æ›´æ–°åˆ†éš”çº¿ ==================== -->\n<!-- ğŸ“ å¢é‡æ›´æ–°æ—¶é—´: {update_timestamp} -->\n<!-- ä»¥ä¸‹æ˜¯æœ¬æ¬¡å¢é‡æ›´æ–°æ–°å¢çš„è¡¨æ ¼æ–‡æ¡£å†…å®¹ -->\n<!-- ========================================================= -->\n\n"
                        output_file.write(separator)
                        log_message(f"å¢é‡æ›´æ–°æ¨¡å¼ï¼šå·²å†™å…¥ç°æœ‰æ–‡æ¡£å†…å®¹ ({len(existing_doc_content)} å­—ç¬¦)ï¼Œå¹¶æ·»åŠ å¢é‡æ›´æ–°åˆ†éš”æ ‡è¯†")
                    else:
                        log_message(f"å¢é‡æ›´æ–°æ¨¡å¼ï¼šè­¦å‘Š - existing_doc_contentä¸ºç©ºï¼Œè·³è¿‡å†™å…¥ç°æœ‰å†…å®¹")

                if incremental_mode:
                    log_message(f"å¢é‡æ›´æ–°æ¨¡å¼ï¼šå¼€å§‹ç”Ÿæˆ{db_type.upper()}æ•°æ®åº“ {database} çš„æ–‡æ¡£...")
                    if existing_tables:
                        log_message(f"å¢é‡æ›´æ–°æ¨¡å¼ï¼šè·³è¿‡ {len(existing_tables)} ä¸ªå·²å­˜åœ¨çš„è¡¨æ ¼")
                else:
                    log_message(f"å¼€å§‹ç”Ÿæˆ{db_type.upper()}æ•°æ®åº“ {database} çš„æ–‡æ¡£...")
                log_message(f"æ–‡æ¡£å°†ä¿å­˜åˆ°: {output_file_path}")

                total_tables = len(selected_tables_final)
                completed_tables = 0

                if total_tables == 0:
                    if incremental_mode:
                        log_message("å¢é‡æ›´æ–°æ¨¡å¼ï¼šæ²¡æœ‰éœ€è¦ç”Ÿæˆçš„æ–°è¡¨æ ¼")
                    else:
                        log_message("æ²¡æœ‰éœ€è¦ç”Ÿæˆçš„æ–°è¡¨æ ¼")
                    output_file.close()
                    connection.close()
                    log_queue.put("GENERATION_COMPLETE:" + output_file_path)
                    return

                for i, table_info in enumerate(selected_tables_final, 1):
                    try:
                        # æå–è¡¨åï¼ˆå¦‚æœæ˜¯å…ƒç»„åˆ™å–ç¬¬ä¸€ä¸ªå…ƒç´ ï¼Œå¦‚æœæ˜¯å­—ç¬¦ä¸²åˆ™ç›´æ¥ä½¿ç”¨ï¼‰
                        if isinstance(table_info, (list, tuple)):
                            table_name = table_info[0]
                        else:
                            table_name = table_info
                            
                        log_message(f"æ­£åœ¨å¤„ç†è¡¨: {table_name} ({i}/{total_tables})")
                        print(f"å¼€å§‹å¤„ç†è¡¨: {table_name}")
                        
                        columns_info = get_columns_info(connection, table_name, database, db_type)
                        print(f"è·å–åˆ°è¡¨ {table_name} çš„ {len(columns_info)} ä¸ªåˆ—ä¿¡æ¯")
                        
                        meanings = infer_chinese_meaning(columns_info, table_name, db_description)
                        print(f"AIæ¨æ–­å®Œæˆï¼Œè¡¨ {table_name} è·å¾— {len(meanings)} ä¸ªå­—æ®µå«ä¹‰")
                        
                        markdown = generate_markdown(columns_info, meanings)
                        print(f"Markdownç”Ÿæˆå®Œæˆï¼Œè¡¨ {table_name}")
                        
                        output_file.write(f"è¡¨: {table_name}\n{markdown}\n")
                        output_file.flush()
                        completed_tables += 1
                        log_message(f"è¡¨ {table_name} æ•´ç†å®Œæˆ ({completed_tables}/{total_tables})")
                        log_queue.put(f"PROGRESS:{completed_tables}:{total_tables}:{table_name}")
                        print(f"è¡¨ {table_name} å¤„ç†å®Œæˆ")
                        
                    except Exception as table_error:
                        import traceback
                        error_msg = f"å¤„ç†è¡¨ {table_name} æ—¶å‡ºé”™: {str(table_error)}"
                        print(error_msg)
                        print(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
                        log_message(error_msg)
                        # ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªè¡¨ï¼Œä¸ä¸­æ–­æ•´ä¸ªæµç¨‹
                        continue

                output_file.close()
                connection.close()
                log_message("æ‰€æœ‰è¡¨æ ¼æ•´ç†å®Œæˆï¼Œæ–‡æ¡£ç”ŸæˆæˆåŠŸï¼")
                log_queue.put("GENERATION_COMPLETE:" + output_file_path)
            except Exception as e:
                log_message(f"ç”Ÿæˆè¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
                log_queue.put("GENERATION_ERROR:" + str(e))

        thread = threading.Thread(target=generate_in_background)
        thread.daemon = True
        thread.start()
        return jsonify({"success": True, "message": "å¼€å§‹ç”Ÿæˆæ–‡æ¡£ï¼Œè¯·æŸ¥çœ‹æ—¥å¿—è¿›åº¦"})
    except Exception as e:
        return jsonify({"success": False, "message": str(e)})


@api_bp.route('/logs')
def logs():
    """è·å–æ—¥å¿—æµ"""
    def generate_logs():
        while True:
            try:
                message = log_queue.get(timeout=1)
                if message.startswith("GENERATION_COMPLETE:"):
                    file_path = message.split(":", 1)[1]
                    yield f"data: {json.dumps({'type': 'complete', 'file_path': file_path})}\n\n"
                    break
                elif message.startswith("GENERATION_ERROR:"):
                    error_msg = message.split(":", 1)[1]
                    yield f"data: {json.dumps({'type': 'error', 'message': error_msg})}\n\n"
                    break
                elif message.startswith("PROGRESS:"):
                    parts = message.split(":", 3)
                    if len(parts) >= 4:
                        completed = int(parts[1])
                        total = int(parts[2])
                        current_table = parts[3]
                        yield f"data: {json.dumps({'type': 'progress', 'completed': completed, 'total': total, 'current_table': current_table})}\n\n"
                else:
                    yield f"data: {json.dumps({'type': 'log', 'message': message})}\n\n"
            except queue.Empty:
                yield f"data: {json.dumps({'type': 'heartbeat'})}\n\n"
                continue
    return Response(generate_logs(), mimetype='text/event-stream')


@api_bp.route('/download/<path:file_path>')
def download_file(file_path):
    """ä¸‹è½½æ–‡ä»¶"""
    try:
        if os.path.exists(file_path):
            return send_file(file_path, as_attachment=True, download_name=f"database_docs_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md")
        else:
            return jsonify({"success": False, "message": "æ–‡ä»¶ä¸å­˜åœ¨"})
    except Exception as e:
        return jsonify({"success": False, "message": str(e)})


@api_bp.route('/preview/<path:file_path>')
def preview_file(file_path):
    """é¢„è§ˆæ–‡ä»¶"""
    try:
        if os.path.exists(file_path):
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            return jsonify({"success": True, "content": content})
        else:
            return jsonify({"success": False, "message": "æ–‡ä»¶ä¸å­˜åœ¨"})
    except Exception as e:
        return jsonify({"success": False, "message": str(e)})