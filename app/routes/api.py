"""
APIè·¯ç”±æ¨¡å—
"""

from flask import Blueprint, request, jsonify, Response, send_file
import os
import json
import queue
from datetime import datetime
import threading
from datetime import datetime
from pathlib import Path
import tkinter as tk
from tkinter import filedialog
import subprocess
import sys

from ..utils import (
    connect_db,
    get_tables_and_views,
    get_tables_with_missing_stats,
    get_columns_info,
    get_foreign_keys,
    get_all_columns,
    infer_chinese_meaning,
    generate_markdown,
    get_databases,
    update_table_comment,
    update_column_comment,
)
from ..utils.ai_helper import get_openai_client
from ..config import config
from .api_graph_mermaid import graph_mermaid

api_bp = Blueprint('api', __name__, url_prefix='/api')

# å…¨å±€æ—¥å¿—é˜Ÿåˆ—
log_queue = queue.Queue()


def log_message(message):
    """è®°å½•æ—¥å¿—æ¶ˆæ¯"""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    log_queue.put(f"[{timestamp}] {message}")


@api_bp.route('/test_connection', methods=['POST'])
def test_connection():
    """æµ‹è¯•æ•°æ®åº“è¿æ¥"""
    try:
        data = request.get_json()
        host = data.get('host')
        user = data.get('user')
        password = data.get('password')
        port = int(data.get('port', 3306 if data.get('db_type', 'mysql') == 'mysql' else 1433))
        database = data.get('database')
        db_type = data.get('db_type', 'mysql')

        connection = connect_db(host, user, password, port, database, db_type)
        tables = get_tables_and_views(connection, database, db_type)
        connection.close()
        return jsonify({"success": True, "tables": tables, "db_type": db_type})
    except Exception as e:
        return jsonify({"success": False, "message": str(e)})


@api_bp.route('/get_tables', methods=['POST'])
def get_tables():
    """è·å–æ•°æ®åº“è¡¨åˆ—è¡¨"""
    try:
        data = request.get_json()
        host = data.get('host')
        user = data.get('user')
        password = data.get('password')
        port = int(data.get('port', 3306 if data.get('db_type', 'mysql') == 'mysql' else 1433))
        database = data.get('database')
        db_type = data.get('db_type', 'mysql')

        connection = connect_db(host, user, password, port, database, db_type)
        # Phase2ï¼šä¸€æ¬¡æ€§è¿”å›è¡¨æ³¨é‡Šä¸â€œå¾…è¡¥å……â€ç»Ÿè®¡ï¼Œé¿å…å‰ç«¯é€è¡¨æ‹‰å–è¯¦æƒ…
        tables = get_tables_with_missing_stats(connection, database, db_type)
        connection.close()
        
        return jsonify({"success": True, "tables": tables})
    except Exception as e:
        return jsonify({"success": False, "message": str(e)})


@api_bp.route('/select_markdown_file', methods=['GET'])
def api_select_markdown_file():
    """é€‰æ‹© Markdown æ–‡ä»¶ï¼ˆç”¨äºå¢é‡æ›´æ–°æ¨¡å¼é€‰æ‹©å·²æœ‰æ–‡æ¡£ï¼‰"""
    try:
        root = tk.Tk()
        root.withdraw()
        root.attributes('-topmost', True)
        file_path = filedialog.askopenfilename(
            title='é€‰æ‹©è¦å¢é‡æ›´æ–°çš„ Markdown æ–‡æ¡£',
            filetypes=[('Markdown', '*.md *.markdown'), ('All Files', '*.*')]
        )
        root.destroy()
        if file_path:
            return jsonify({"success": True, "path": file_path})
        return jsonify({"success": False, "message": "ç”¨æˆ·å–æ¶ˆ"})
    except Exception as e:
        return jsonify({"success": False, "message": str(e)})


@api_bp.route('/list_databases', methods=['POST'])
def list_databases():
    """è·å–æœåŠ¡å™¨ä¸Šçš„æ•°æ®åº“åˆ—è¡¨"""
    try:
        data = request.get_json()
        host = data.get('host')
        user = data.get('user')
        password = data.get('password')
        db_type = data.get('db_type', 'mysql')
        # æ ¹æ®æ•°æ®åº“ç±»å‹è®¾ç½®é»˜è®¤ç«¯å£
        port = int(data.get('port', 3306 if db_type == 'mysql' else 1433))

        databases = get_databases(host, user, password, port, db_type)
        return jsonify({"success": True, "databases": databases})
    except Exception as e:
        return jsonify({"success": False, "message": str(e)})


@api_bp.route('/get_table_detail', methods=['POST'])
def get_table_detail():
    """è·å–è¡¨çš„è¯¦ç»†ä¿¡æ¯"""
    try:
        data = request.get_json()
        host = data.get('host')
        user = data.get('user')
        password = data.get('password')
        port = int(data.get('port', 3306 if data.get('db_type', 'mysql') == 'mysql' else 1433))
        database = data.get('database')
        db_type = data.get('db_type', 'mysql')
        table_name = data.get('table_name')

        connection = connect_db(host, user, password, port, database, db_type)
        columns_info = get_columns_info(connection, table_name, database, db_type)
        connection.close()

        # å¤„ç†è¡¨æ³¨é‡Š
        table_comment = ''
        if db_type == 'mysql':
            # å•ç‹¬æŸ¥è¯¢è¡¨æ³¨é‡Š
            connection = connect_db(host, user, password, port, database, db_type)
            cursor = connection.cursor()
            query = """
            SELECT table_comment 
            FROM information_schema.tables 
            WHERE table_schema = %s AND table_name = %s
            """
            cursor.execute(query, (database, table_name))
            table_comment_result = cursor.fetchone()
            if table_comment_result:
                table_comment = table_comment_result[0]
            cursor.close()
            connection.close()
        elif db_type == 'sqlserver':
            # å•ç‹¬æŸ¥è¯¢è¡¨æ³¨é‡Š
            connection = connect_db(host, user, password, port, database, db_type)
            cursor = connection.cursor()
            query = """
            SELECT ISNULL(CAST(ep.value AS NVARCHAR(MAX)), '') as table_comment
            FROM sys.tables t
            LEFT JOIN sys.extended_properties ep ON ep.major_id = t.object_id AND ep.minor_id = 0 AND ep.name = 'MS_Description'
            WHERE t.name = ?
            """
            cursor.execute(query, (table_name,))
            table_comment_result = cursor.fetchone()
            if table_comment_result:
                table_comment = table_comment_result[0]
            cursor.close()
            connection.close()

        # æ ¼å¼åŒ–åˆ—ä¿¡æ¯
        fields = []
        for column in columns_info:
            if db_type == 'mysql':
                # MySQLæ ¼å¼: (column_name, data_type, is_nullable, column_default, column_comment, character_maximum_length, numeric_precision, numeric_scale)
                field = {
                    'name': column[0],
                    'type': column[1],
                    'nullable': column[2] == 'YES',
                    'default': column[3],
                    'comment': column[4]
                }
            elif db_type == 'sqlserver':
                # SQL Serveræ ¼å¼: (column_name, data_type, is_nullable, column_default, column_comment, character_maximum_length, numeric_precision, numeric_scale)
                field = {
                    'name': column[0],
                    'type': column[1],
                    'nullable': column[2] == 'YES',
                    'default': column[3],
                    'comment': column[4]
                }
            fields.append(field)

        return jsonify({"success": True, "table_info": {"comment": table_comment}, "fields": fields})
    except Exception as e:
        return jsonify({"success": False, "message": str(e)})


@api_bp.route('/generate_field_description', methods=['POST'])
def generate_field_description():
    """ç”Ÿæˆå•ä¸ªå­—æ®µè¯´æ˜"""
    try:
        data = request.get_json()
        host = data.get('host')
        user = data.get('user')
        password = data.get('password')
        port = int(data.get('port', 3306 if data.get('db_type', 'mysql') == 'mysql' else 1433))
        database = data.get('database')
        db_type = data.get('db_type', 'mysql')
        table_name = data.get('table_name')
        field_name = data.get('field_name')
        db_description = data.get('db_description', '')

        # è¿æ¥æ•°æ®åº“è·å–è¡¨ç»“æ„ä¿¡æ¯
        connection = connect_db(host, user, password, port, database, db_type)
        columns_info = get_columns_info(connection, table_name, database, db_type)
        connection.close()

        # æŸ¥æ‰¾æŒ‡å®šå­—æ®µçš„ä¿¡æ¯
        field_info = None
        for column in columns_info:
            if column[0] == field_name:
                field_info = column
                break

        if not field_info:
            return jsonify({"success": False, "message": f"å­—æ®µ {field_name} ä¸å­˜åœ¨"})

        # åªä¼ å…¥éœ€è¦ç”Ÿæˆè¯´æ˜çš„å­—æ®µ
        single_field_info = [field_info]
        meanings = infer_chinese_meaning(single_field_info, table_name, db_description)

        # è·å–ç”Ÿæˆçš„å­—æ®µè¯´æ˜
        field_description = meanings.get(field_name, '')

        return jsonify({"success": True, "field_name": field_name, "field_description": field_description})
    except Exception as e:
        return jsonify({"success": False, "message": str(e)})


@api_bp.route('/generate_all_fields_description', methods=['POST'])
def generate_all_fields_description():
    """ç”Ÿæˆæ‰€æœ‰å­—æ®µè¯´æ˜"""
    try:
        data = request.get_json()
        host = data.get('host')
        user = data.get('user')
        password = data.get('password')
        port = int(data.get('port', 3306 if data.get('db_type', 'mysql') == 'mysql' else 1433))
        database = data.get('database')
        db_type = data.get('db_type', 'mysql')
        table_name = data.get('table_name')
        db_description = data.get('db_description', '')

        # è¿æ¥æ•°æ®åº“è·å–è¡¨ç»“æ„ä¿¡æ¯
        connection = connect_db(host, user, password, port, database, db_type)
        columns_info = get_columns_info(connection, table_name, database, db_type)
        connection.close()

        # ç”Ÿæˆæ‰€æœ‰å­—æ®µçš„è¯´æ˜
        meanings = infer_chinese_meaning(columns_info, table_name, db_description)

        return jsonify({"success": True, "field_meanings": meanings})
    except Exception as e:
        return jsonify({"success": False, "message": str(e)})


@api_bp.route('/generate_table_description', methods=['POST'])
def generate_table_description():
    """ç”Ÿæˆè¡¨è¯´æ˜"""
    try:
        data = request.get_json()
        host = data.get('host')
        user = data.get('user')
        password = data.get('password')
        port = int(data.get('port', 3306 if data.get('db_type', 'mysql') == 'mysql' else 1433))
        database = data.get('database')
        db_type = data.get('db_type', 'mysql')
        table_name = data.get('table_name')
        db_description = data.get('db_description', '')

        # è¿æ¥æ•°æ®åº“è·å–è¡¨ç»“æ„ä¿¡æ¯
        connection = connect_db(host, user, password, port, database, db_type)
        columns_info = get_columns_info(connection, table_name, database, db_type)
        connection.close()

        # ç”Ÿæˆè¡¨è¯´æ˜
        # è¿™é‡Œæˆ‘ä»¬å¤ç”¨ infer_chinese_meaning å‡½æ•°ï¼Œé€šè¿‡ä¿®æ”¹æç¤ºè¯æ¥ç”Ÿæˆè¡¨è¯´æ˜
        client = get_openai_client()
        if not client:
            return jsonify({"success": False, "message": "AIå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥"})

        ai_config = config.get('ai', {}).get('openai', {})
        model = ai_config.get('model', 'google/gemma-3-1b')

        # æ„å»ºåˆ—ä¿¡æ¯å­—ç¬¦ä¸²
        column_info = []
        for col in columns_info:
            column_name = str(col[0]) if col[0] is not None else ''
            data_type = str(col[1]) if col[1] is not None else ''
            column_info.append(f"åˆ—å: {column_name}, ç±»å‹: {data_type}")

        columns_text = "\n".join(column_info)
        table_name_str = str(table_name) if table_name is not None else "æœªçŸ¥è¡¨"

        # æ„å»ºåŒ…å«æ•°æ®åº“åŠŸèƒ½ä»‹ç»çš„æç¤ºè¯
        db_context = ""
        if db_description and db_description.strip():
            db_context = f"\n\nå‚è€ƒä¿¡æ¯ï¼šæ•°æ®åº“åŠŸèƒ½ä»‹ç»ï¼š\n{db_description.strip()}\n\nè¯·ç»“åˆä»¥ä¸Šæ•°æ®åº“åŠŸèƒ½ä»‹ç»æ¥æ¨æ–­è¡¨çš„åŠŸèƒ½å’Œç”¨é€”ã€‚"

        prompt = f"""è¯·ç»™å‡ºè¡¨ {table_name_str} çš„ä¸­æ–‡è¯´æ˜ï¼Œè¦æ±‚è¯´æ˜è¡¨çš„åŠŸèƒ½ã€ç”¨é€”å’Œä¸»è¦ä¸šåŠ¡åœºæ™¯ï¼Œä¸è¶…è¿‡100ä¸ªå­—ç¬¦ã€‚\n\nè¡¨ç»“æ„ä¿¡æ¯ï¼š\n{columns_text}{db_context}"""

        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæ•°æ®åº“ä¸“å®¶ï¼Œæ“…é•¿æ ¹æ®è¡¨ç»“æ„å’Œä¸šåŠ¡åœºæ™¯æ¨æ–­è¡¨çš„åŠŸèƒ½å’Œç”¨é€”ã€‚"},
                {"role": "user", "content": prompt}
            ],
            max_tokens=100,
            temperature=0.3
        )

        table_description = response.choices[0].message.content.strip()

        return jsonify({"success": True, "table_description": table_description})
    except Exception as e:
        return jsonify({"success": False, "message": str(e)})


@api_bp.route('/save_table_comment', methods=['POST'])
def save_table_comment():
    """ä¿å­˜è¡¨æ³¨é‡Šå’Œå­—æ®µæ³¨é‡Šåˆ°æ•°æ®åº“"""
    try:
        data = request.get_json()
        host = data.get('host')
        user = data.get('user')
        password = data.get('password')
        port = int(data.get('port', 3306 if data.get('db_type', 'mysql') == 'mysql' else 1433))
        database = data.get('database')
        db_type = data.get('db_type', 'mysql')
        table_name = data.get('table_name')
        table_description = data.get('table_description', '')
        field_descriptions = data.get('field_descriptions', {})

        # è¿æ¥æ•°æ®åº“
        connection = connect_db(host, user, password, port, database, db_type)

        try:
            # æ›´æ–°è¡¨æ³¨é‡Š
            update_table_comment(connection, table_name, database, table_description, db_type)
            
            # æ›´æ–°å­—æ®µæ³¨é‡Š
            for field_name, field_desc in field_descriptions.items():
                update_column_comment(connection, table_name, database, field_name, field_desc, db_type)
            
            connection.close()
            return jsonify({"success": True, "message": "è¡¨æ³¨é‡Šå’Œå­—æ®µæ³¨é‡Šä¿å­˜æˆåŠŸ"})
        except Exception as e:
            connection.close()
            return jsonify({"success": False, "message": f"ä¿å­˜å¤±è´¥: {str(e)}"})
    except Exception as e:
        return jsonify({"success": False, "message": f"æ•°æ®åº“è¿æ¥å¤±è´¥: {str(e)}"})


@api_bp.route('/generate_all_tables_description', methods=['POST'])
def generate_all_tables_description():
    """ä¸€é”®ç”Ÿæˆæ‰€æœ‰è¡¨å’Œå­—æ®µçš„æè¿°å¹¶æ›´æ–°åˆ°æ•°æ®åº“"""
    try:
        data = request.get_json()
        host = data.get('host')
        user = data.get('user')
        password = data.get('password')
        port = int(data.get('port', 3306 if data.get('db_type', 'mysql') == 'mysql' else 1433))
        database = data.get('database')
        db_type = data.get('db_type', 'mysql')
        db_description = data.get('db_description', '')

        # è¿æ¥æ•°æ®åº“
        connection = connect_db(host, user, password, port, database, db_type)
        
        try:
            # è·å–æ‰€æœ‰è¡¨åˆ—è¡¨
            tables = get_tables_and_views(connection, database, db_type)
            
            # å¤„ç†ç»“æœ
            tables_list = []
            if isinstance(tables[0], (list, tuple)):
                # æ ¼å¼ä¸º [(table_name, table_type, table_comment), ...]
                tables_list = [table[0] for table in tables]
            else:
                # æ ¼å¼ä¸º [table_name, table_name, ...]
                tables_list = tables
            
            # ç”Ÿæˆç»“æœç»Ÿè®¡
            result = {
                "success": True,
                "total_tables": len(tables_list),
                "processed_tables": 0,
                "success_tables": 0,
                "failed_tables": 0,
                "failed_table_details": []
            }
            
            # éå†æ‰€æœ‰è¡¨ï¼Œç”Ÿæˆæè¿°å¹¶ä¿å­˜
            total_tables = len(tables_list)
            for index, table_name in enumerate(tables_list, 1):
                try:
                    result["processed_tables"] += 1
                    
                    # æ›´æ–°å½“å‰çŠ¶æ€
                    current_status = f"æ­£åœ¨å¤„ç†è¡¨: {table_name} ({index}/{total_tables})"
                    log_message(current_status)
                    
                    # æ¨é€è¿›åº¦ä¿¡æ¯
                    progress_percent = int((index / total_tables) * 100)
                    log_queue.put(f"PROGRESS:{index}:{total_tables}:{table_name}")
                    
                    # 1. ç”Ÿæˆè¡¨è¯´æ˜
                    table_gen_response = generate_table_description_internal(connection, table_name, database, db_type, db_description)
                    table_description = table_gen_response["table_description"]
                    
                    # 2. ç”Ÿæˆæ‰€æœ‰å­—æ®µè¯´æ˜
                    columns_info = get_columns_info(connection, table_name, database, db_type)
                    field_meanings = infer_chinese_meaning(columns_info, table_name, db_description)
                    
                    # 3. ä¿å­˜åˆ°æ•°æ®åº“
                    update_table_comment(connection, table_name, database, table_description, db_type)
                    
                    for field_name, field_desc in field_meanings.items():
                        update_column_comment(connection, table_name, database, field_name, field_desc, db_type)
                    
                    result["success_tables"] += 1
                    log_message(f"æˆåŠŸç”Ÿæˆå¹¶ä¿å­˜è¡¨ {table_name} çš„æè¿°")
                except Exception as e:
                    result["failed_tables"] += 1
                    result["failed_table_details"].append({
                        "table_name": table_name,
                        "error": str(e)
                    })
                    log_message(f"å¤„ç†è¡¨ {table_name} æ—¶å‡ºé”™: {str(e)}")
                    continue
            
            connection.close()
            return jsonify(result)
        except Exception as e:
            connection.close()
            return jsonify({"success": False, "message": f"æ‰¹é‡ç”Ÿæˆå¤±è´¥: {str(e)}"})
    except Exception as e:
        return jsonify({"success": False, "message": f"æ•°æ®åº“è¿æ¥å¤±è´¥: {str(e)}"})


def generate_table_description_internal(connection, table_name, database, db_type, db_description):
    """å†…éƒ¨å‡½æ•°ï¼šç”Ÿæˆè¡¨è¯´æ˜"""
    columns_info = get_columns_info(connection, table_name, database, db_type)
    
    client = get_openai_client()
    if not client:
        raise Exception("AIå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥")

    ai_config = config.get('ai', {}).get('openai', {})
    model = ai_config.get('model', 'google/gemma-3-1b')

    # æ„å»ºåˆ—ä¿¡æ¯å­—ç¬¦ä¸²
    column_info = []
    for col in columns_info:
        column_name = str(col[0]) if col[0] is not None else ''
        data_type = str(col[1]) if col[1] is not None else ''
        column_info.append(f"åˆ—å: {column_name}, ç±»å‹: {data_type}")

    columns_text = "\n".join(column_info)
    table_name_str = str(table_name) if table_name is not None else "æœªçŸ¥è¡¨"

    # æ„å»ºåŒ…å«æ•°æ®åº“åŠŸèƒ½ä»‹ç»çš„æç¤ºè¯
    db_context = ""
    if db_description and db_description.strip():
        db_context = f"\n\nå‚è€ƒä¿¡æ¯ï¼šæ•°æ®åº“åŠŸèƒ½ä»‹ç»ï¼š\n{db_description.strip()}\n\nè¯·ç»“åˆä»¥ä¸Šæ•°æ®åº“åŠŸèƒ½ä»‹ç»æ¥æ¨æ–­è¡¨çš„åŠŸèƒ½å’Œç”¨é€”ã€‚"

    prompt = f"""è¯·ç»™å‡ºè¡¨ {table_name_str} çš„ä¸­æ–‡è¯´æ˜ï¼Œè¦æ±‚è¯´æ˜è¡¨çš„åŠŸèƒ½ã€ç”¨é€”å’Œä¸»è¦ä¸šåŠ¡åœºæ™¯ï¼Œä¸è¶…è¿‡100ä¸ªå­—ç¬¦ã€‚\n\nè¡¨ç»“æ„ä¿¡æ¯ï¼š\n{columns_text}{db_context}"""

    response = client.chat.completions.create(
        model=model,
        messages=[
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæ•°æ®åº“ä¸“å®¶ï¼Œæ“…é•¿æ ¹æ®è¡¨ç»“æ„å’Œä¸šåŠ¡åœºæ™¯æ¨æ–­è¡¨çš„åŠŸèƒ½å’Œç”¨é€”ã€‚"},
            {"role": "user", "content": prompt}
        ],
        max_tokens=100,
        temperature=0.3
    )

    table_description = response.choices[0].message.content.strip()
    
    return {"table_description": table_description}


@api_bp.route('/parse_doc', methods=['POST'])
def parse_doc():
    """è§£ææ–‡æ¡£"""
    try:
        data = request.get_json()
        file_path = data.get('doc_path')
        if not file_path or not os.path.exists(file_path):
            return jsonify({"success": False, "message": "æ–‡æ¡£è·¯å¾„æ— æ•ˆæˆ–æ–‡ä»¶ä¸å­˜åœ¨"})
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        return jsonify({"success": True, "content": content})
    except Exception as e:
        return jsonify({"success": False, "message": str(e)})


@api_bp.route('/default_path', methods=['GET'])
def api_default_path():
    """è·å–é»˜è®¤è·¯å¾„"""
    def get_downloads_dir():
        # Windowsä¼˜å…ˆä½¿ç”¨ç”¨æˆ·ä¸»ç›®å½• + Downloadsï¼Œè‹¥ä¸å­˜åœ¨åˆ™é€€å›ä¸»ç›®å½•
        downloads = Path.home() / 'Downloads'
        return str(downloads if downloads.exists() else Path.home())

    try:
        return jsonify({"success": True, "path": get_downloads_dir()})
    except Exception as e:
        return jsonify({"success": False, "message": str(e)})


@api_bp.route('/select_directory', methods=['GET'])
def api_select_directory():
    """é€‰æ‹©ç›®å½•"""
    try:
        root = tk.Tk()
        root.withdraw()
        root.attributes('-topmost', True)
        path = filedialog.askdirectory(title='é€‰æ‹©ä¿å­˜æ–‡ä»¶å¤¹')
        root.destroy()
        if path:
            return jsonify({"success": True, "path": path})
        else:
            return jsonify({"success": False, "message": "ç”¨æˆ·å–æ¶ˆ"})
    except Exception as e:
        return jsonify({"success": False, "message": str(e)})


@api_bp.route('/generate_docs', methods=['POST'])
def generate_docs():
    """ç”Ÿæˆæ–‡æ¡£"""
    try:
        data = request.get_json()
        host = data.get('host')
        user = data.get('user')
        password = data.get('password')
        port = int(data.get('port', 3306 if data.get('db_type', 'mysql') == 'mysql' else 1433))
        database = data.get('database')
        db_type = data.get('db_type', 'mysql')
        selected_tables = data.get('tables', [])
        output_path = data.get('output_path', '')
        file_name = data.get('file_name', f'{database}_æ–‡æ¡£_{datetime.now().strftime("%Y%m%d_%H%M%S")}.md')

        incremental_mode = data.get('incremental_mode', False)
        existing_doc_path = data.get('existing_doc_path', '')
        existing_doc_content = data.get('existing_doc_content', '')
        existing_tables = data.get('existing_tables', [])
        db_description = data.get('db_description', '')
        
        # è°ƒè¯•æ—¥å¿—
        if incremental_mode:
            log_message(f"å¢é‡æ¨¡å¼è°ƒè¯• - existing_doc_path: {existing_doc_path}")
            log_message(f"å¢é‡æ¨¡å¼è°ƒè¯• - existing_doc_contenté•¿åº¦: {len(existing_doc_content) if existing_doc_content else 0}")
            log_message(f"å¢é‡æ¨¡å¼è°ƒè¯• - existing_tables: {existing_tables}")

        # æ¸…ç©ºæ—¥å¿—é˜Ÿåˆ—
        while not log_queue.empty():
            log_queue.get()

        def generate_in_background():
            try:
                connection = connect_db(host, user, password, port, database, db_type)

                if incremental_mode and existing_tables:
                    new_tables = [table for table in selected_tables if table not in existing_tables]
                    skipped_count = len(selected_tables) - len(new_tables)
                    if skipped_count > 0:
                        log_message(f"å¢é‡æ›´æ–°æ¨¡å¼ï¼šè·³è¿‡ {skipped_count} ä¸ªå·²å­˜åœ¨çš„è¡¨æ ¼")
                    selected_tables_final = new_tables
                else:
                    selected_tables_final = selected_tables

                # ç¡®å®šè¾“å‡ºæ–‡ä»¶è·¯å¾„
                if incremental_mode and existing_doc_path:
                    # å¢é‡æ›´æ–°æ¨¡å¼ï¼šç”Ÿæˆå¸¦æœ‰å¢é‡æ ‡è¯†çš„æ–°æ–‡ä»¶å
                    base_name = os.path.splitext(existing_doc_path)[0]  # å»æ‰æ‰©å±•å
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    incremental_file_name = f"{base_name}_å¢é‡æ›´æ–°_{timestamp}.md"
                    
                    if output_path:
                        user_output_path = output_path
                        if not os.path.isabs(user_output_path):
                            user_output_path = os.path.abspath(user_output_path)
                        if not os.path.exists(user_output_path):
                            os.makedirs(user_output_path, exist_ok=True)
                            log_message(f"åˆ›å»ºè¾“å‡ºç›®å½•: {user_output_path}")
                        output_file_path = os.path.join(user_output_path, incremental_file_name)
                        log_message(f"å¢é‡æ›´æ–°æ¨¡å¼ï¼šå°†æ›´æ–°æ–‡æ¡£ä¿å­˜åˆ°: {output_file_path}")
                    else:
                        default_output_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', '..', 'data', 'output')
                        if not os.path.exists(default_output_dir):
                            os.makedirs(default_output_dir, exist_ok=True)
                            log_message(f"åˆ›å»ºé»˜è®¤è¾“å‡ºç›®å½•: {default_output_dir}")
                        output_file_path = os.path.join(default_output_dir, incremental_file_name)
                        log_message(f"å¢é‡æ›´æ–°æ¨¡å¼ï¼šå°†æ›´æ–°æ–‡æ¡£ä¿å­˜åˆ°é»˜è®¤è·¯å¾„: {output_file_path}")
                    output_file = open(output_file_path, 'w', encoding='utf-8')
                else:
                    # æ™®é€šæ¨¡å¼ï¼šä½¿ç”¨ç”Ÿæˆçš„æ–‡ä»¶å
                    if output_path:
                        user_output_path = output_path
                        if not os.path.isabs(user_output_path):
                            user_output_path = os.path.abspath(user_output_path)
                        if not os.path.exists(user_output_path):
                            os.makedirs(user_output_path, exist_ok=True)
                            log_message(f"åˆ›å»ºè¾“å‡ºç›®å½•: {user_output_path}")
                        output_file_path = os.path.join(user_output_path, file_name)
                        output_file = open(output_file_path, 'w', encoding='utf-8')
                        log_message(f"æ–‡æ¡£å°†ä¿å­˜åˆ°: {output_file_path}")
                    else:
                        default_output_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', '..', 'data', 'output')
                        if not os.path.exists(default_output_dir):
                            os.makedirs(default_output_dir, exist_ok=True)
                            log_message(f"åˆ›å»ºé»˜è®¤è¾“å‡ºç›®å½•: {default_output_dir}")
                        output_file_path = os.path.join(default_output_dir, file_name)
                        output_file = open(output_file_path, 'w', encoding='utf-8')
                        log_message(f"ä½¿ç”¨é»˜è®¤è·¯å¾„ä¿å­˜æ–‡æ¡£: {output_file_path}")

                # åœ¨å¢é‡æ›´æ–°æ¨¡å¼ä¸‹ï¼Œå…ˆå†™å…¥ç°æœ‰æ–‡æ¡£å†…å®¹
                if incremental_mode:
                    log_message(f"å¢é‡æ›´æ–°æ¨¡å¼ï¼šæ£€æŸ¥ç°æœ‰æ–‡æ¡£å†…å®¹...")
                    log_message(f"existing_doc_contentæ˜¯å¦å­˜åœ¨: {existing_doc_content is not None}")
                    log_message(f"existing_doc_contenté•¿åº¦: {len(existing_doc_content) if existing_doc_content else 0}")
                    
                    if existing_doc_content:
                        log_message(f"å¢é‡æ›´æ–°æ¨¡å¼ï¼šå¼€å§‹å†™å…¥ç°æœ‰æ–‡æ¡£å†…å®¹...")
                        output_file.write(existing_doc_content)
                        # ç¡®ä¿ç°æœ‰å†…å®¹åæœ‰æ¢è¡Œç¬¦
                        if not existing_doc_content.endswith('\n'):
                            output_file.write('\n')
                        
                        # æ·»åŠ å¢é‡æ›´æ–°åˆ†éš”æ ‡è¯†
                        update_timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                        separator = f"\n\n<!-- ==================== å¢é‡æ›´æ–°åˆ†éš”çº¿ ==================== -->\n<!-- ğŸ“ å¢é‡æ›´æ–°æ—¶é—´: {update_timestamp} -->\n<!-- ä»¥ä¸‹æ˜¯æœ¬æ¬¡å¢é‡æ›´æ–°æ–°å¢çš„è¡¨æ ¼æ–‡æ¡£å†…å®¹ -->\n<!-- ========================================================= -->\n\n"
                        output_file.write(separator)
                        log_message(f"å¢é‡æ›´æ–°æ¨¡å¼ï¼šå·²å†™å…¥ç°æœ‰æ–‡æ¡£å†…å®¹ ({len(existing_doc_content)} å­—ç¬¦)ï¼Œå¹¶æ·»åŠ å¢é‡æ›´æ–°åˆ†éš”æ ‡è¯†")
                    else:
                        log_message(f"å¢é‡æ›´æ–°æ¨¡å¼ï¼šè­¦å‘Š - existing_doc_contentä¸ºç©ºï¼Œè·³è¿‡å†™å…¥ç°æœ‰å†…å®¹")

                if incremental_mode:
                    log_message(f"å¢é‡æ›´æ–°æ¨¡å¼ï¼šå¼€å§‹ç”Ÿæˆ{db_type.upper()}æ•°æ®åº“ {database} çš„æ–‡æ¡£...")
                    if existing_tables:
                        log_message(f"å¢é‡æ›´æ–°æ¨¡å¼ï¼šè·³è¿‡ {len(existing_tables)} ä¸ªå·²å­˜åœ¨çš„è¡¨æ ¼")
                else:
                    log_message(f"å¼€å§‹ç”Ÿæˆ{db_type.upper()}æ•°æ®åº“ {database} çš„æ–‡æ¡£...")
                log_message(f"æ–‡æ¡£å°†ä¿å­˜åˆ°: {output_file_path}")

                total_tables = len(selected_tables_final)
                completed_tables = 0

                if total_tables == 0:
                    if incremental_mode:
                        log_message("å¢é‡æ›´æ–°æ¨¡å¼ï¼šæ²¡æœ‰éœ€è¦ç”Ÿæˆçš„æ–°è¡¨æ ¼")
                    else:
                        log_message("æ²¡æœ‰éœ€è¦ç”Ÿæˆçš„æ–°è¡¨æ ¼")
                    output_file.close()
                    connection.close()
                    log_queue.put("GENERATION_COMPLETE:" + output_file_path)
                    return

                for i in range(len(selected_tables_final)):
                    table_info = selected_tables_final[i]
                    table_index = i + 1
                    try:
                        # æå–è¡¨åï¼ˆå¦‚æœæ˜¯å…ƒç»„åˆ™å–ç¬¬ä¸€ä¸ªå…ƒç´ ï¼Œå¦‚æœæ˜¯å­—ç¬¦ä¸²åˆ™ç›´æ¥ä½¿ç”¨ï¼‰
                        if isinstance(table_info, (list, tuple)):
                            table_name = table_info[0]
                        else:
                            table_name = table_info
                            
                        log_message(f"æ­£åœ¨å¤„ç†è¡¨: {table_name} ({table_index}/{total_tables})")
                        print(f"å¼€å§‹å¤„ç†è¡¨: {table_name}")
                        
                        # è·å–åˆ—ä¿¡æ¯ï¼ŒåŒ…å«å­—æ®µæ³¨é‡Š
                        columns_info = get_columns_info(connection, table_name, database, db_type)
                        print(f"è·å–åˆ°è¡¨ {table_name} çš„ {len(columns_info)} ä¸ªåˆ—ä¿¡æ¯")
                        
                        # ç›´æ¥ä»æ•°æ®åº“è¯»å–è¡¨æ³¨é‡Š
                        table_comment = ''
                        if db_type == 'mysql':
                            query = """
                            SELECT table_comment 
                            FROM information_schema.tables 
                            WHERE table_schema = %s AND table_name = %s
                            """
                            cursor = connection.cursor()
                            cursor.execute(query, (database, table_name))
                            table_comment_result = cursor.fetchone()
                            if table_comment_result:
                                table_comment = table_comment_result[0]
                            cursor.close()
                        elif db_type == 'sqlserver':
                            query = """
                            SELECT ISNULL(CAST(ep.value AS NVARCHAR(MAX)), '') as table_comment
                            FROM sys.tables t
                            LEFT JOIN sys.extended_properties ep ON ep.major_id = t.object_id AND ep.minor_id = 0 AND ep.name = 'MS_Description'
                            WHERE t.name = ?
                            """
                            cursor = connection.cursor()
                            cursor.execute(query, (table_name,))
                            table_comment_result = cursor.fetchone()
                            if table_comment_result:
                                table_comment = table_comment_result[0]
                            cursor.close()
                        
                        # æå–å­—æ®µæ³¨é‡Šä½œä¸ºå«ä¹‰
                        meanings = {}
                        for col in columns_info:
                            column_name = str(col[0]) if col[0] is not None else ''
                            column_comment = str(col[4]) if len(col) > 4 and col[4] is not None else ''
                            if column_comment:
                                meanings[column_name] = column_comment
                        print(f"ä»æ•°æ®åº“è¯»å–åˆ° {len(meanings)} ä¸ªå­—æ®µæ³¨é‡Š")
                        
                        # å¦‚æœå­—æ®µæ³¨é‡Šä¸è¶³ï¼Œä½¿ç”¨AIæ¨æ–­è¡¥å……
                        if len(meanings) < len(columns_info):
                            missing_columns = [str(col[0]) for col in columns_info if str(col[0]) not in meanings or not meanings[str(col[0])]]
                            if missing_columns:
                                print(f"è¡¨ {table_name} ç¼ºå°‘ {len(missing_columns)} ä¸ªå­—æ®µæ³¨é‡Šï¼Œä½¿ç”¨AIæ¨æ–­è¡¥å……")
                                ai_meanings = infer_chinese_meaning(columns_info, table_name, db_description)
                                # åˆå¹¶æ•°æ®åº“æ³¨é‡Šå’ŒAIæ¨æ–­ç»“æœï¼Œä¼˜å…ˆä½¿ç”¨æ•°æ®åº“æ³¨é‡Š
                                for col in columns_info:
                                    column_name = str(col[0]) if col[0] is not None else ''
                                    if column_name not in meanings or not meanings[column_name]:
                                        if column_name in ai_meanings:
                                            meanings[column_name] = ai_meanings[column_name]
                        
                        markdown = generate_markdown(columns_info, meanings)
                        print(f"Markdownç”Ÿæˆå®Œæˆï¼Œè¡¨ {table_name}")
                        
                        # å†™å…¥è¡¨è¯´æ˜å’ŒMarkdown
                        if table_comment:
                            output_file.write(f"è¡¨: {table_name} - {table_comment}\n{markdown}\n")
                        else:
                            output_file.write(f"è¡¨: {table_name}\n{markdown}\n")
                            
                        output_file.flush()
                        completed_tables += 1
                        log_message(f"è¡¨ {table_name} æ•´ç†å®Œæˆ ({completed_tables}/{total_tables})")
                        log_queue.put(f"PROGRESS:{completed_tables}:{total_tables}:{table_name}")
                        print(f"è¡¨ {table_name} å¤„ç†å®Œæˆ")
                        
                    except Exception as table_error:
                        import traceback
                        error_msg = f"å¤„ç†è¡¨ {table_name} æ—¶å‡ºé”™: {str(table_error)}"
                        print(error_msg)
                        print(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
                        log_message(error_msg)
                        # ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªè¡¨ï¼Œä¸ä¸­æ–­æ•´ä¸ªæµç¨‹
                        continue

                output_file.close()
                connection.close()
                log_message("æ‰€æœ‰è¡¨æ ¼æ•´ç†å®Œæˆï¼Œæ–‡æ¡£ç”ŸæˆæˆåŠŸï¼")
                log_queue.put("GENERATION_COMPLETE:" + output_file_path)
            except Exception as e:
                log_message(f"ç”Ÿæˆè¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
                log_queue.put("GENERATION_ERROR:" + str(e))

        thread = threading.Thread(target=generate_in_background)
        thread.daemon = True
        thread.start()
        return jsonify({"success": True, "message": "å¼€å§‹ç”Ÿæˆæ–‡æ¡£ï¼Œè¯·æŸ¥çœ‹æ—¥å¿—è¿›åº¦"})
    except Exception as e:
        return jsonify({"success": False, "message": str(e)})


@api_bp.route('/logs')
def logs():
    """è·å–æ—¥å¿—æµ"""
    def generate_logs():
        while True:
            try:
                message = log_queue.get(timeout=1)
                if message.startswith("GENERATION_COMPLETE:"):
                    file_path = message.split(":", 1)[1]
                    yield f"data: {json.dumps({'type': 'complete', 'file_path': file_path})}\n\n"
                    break
                elif message.startswith("GENERATION_ERROR:"):
                    error_msg = message.split(":", 1)[1]
                    yield f"data: {json.dumps({'type': 'error', 'message': error_msg})}\n\n"
                    break
                elif message.startswith("PROGRESS:"):
                    parts = message.split(":", 3)
                    if len(parts) >= 4:
                        completed = int(parts[1])
                        total = int(parts[2])
                        current_table = parts[3]
                        yield f"data: {json.dumps({'type': 'progress', 'completed': completed, 'total': total, 'current_table': current_table})}\n\n"
                else:
                    yield f"data: {json.dumps({'type': 'log', 'message': message})}\n\n"
            except queue.Empty:
                yield f"data: {json.dumps({'type': 'heartbeat'})}\n\n"
                continue
    return Response(generate_logs(), mimetype='text/event-stream')


@api_bp.route('/download/<path:file_path>')
def download_file(file_path):
    """ä¸‹è½½æ–‡ä»¶"""
    try:
        if os.path.exists(file_path):
            return send_file(file_path, as_attachment=True, download_name=f"database_docs_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md")
        else:
            return jsonify({"success": False, "message": "æ–‡ä»¶ä¸å­˜åœ¨"})
    except Exception as e:
        return jsonify({"success": False, "message": str(e)})


@api_bp.route('/preview/<path:file_path>')
def preview_file(file_path):
    """é¢„è§ˆæ–‡ä»¶"""
    try:
        if os.path.exists(file_path):
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            return jsonify({"success": True, "content": content})
        else:
            return jsonify({"success": False, "message": "æ–‡ä»¶ä¸å­˜åœ¨"})
    except Exception as e:
        return jsonify({"success": False, "message": str(e)})


@api_bp.route('/open_path', methods=['POST'])
def open_path():
    """åœ¨æœ¬æœºæ‰“å¼€æ–‡ä»¶æˆ–æ–‡ä»¶å¤¹ï¼ˆWindowsèµ„æºç®¡ç†å™¨ / macOS Finder / Linux æ–‡ä»¶ç®¡ç†å™¨ï¼‰"""
    try:
        data = request.get_json() or {}
        path = data.get('path', '')
        open_parent = bool(data.get('open_parent', False))
        if not path:
            return jsonify({"success": False, "message": "path ä¸èƒ½ä¸ºç©º"})

        # è§„èŒƒåŒ–è·¯å¾„
        target = os.path.abspath(path)
        if open_parent:
            target = os.path.dirname(target)

        if not os.path.exists(target):
            return jsonify({"success": False, "message": "è·¯å¾„ä¸å­˜åœ¨"})

        # Windows
        if os.name == 'nt':
            os.startfile(target)  # noqa: S606
            return jsonify({"success": True})

        # macOS / Linux
        if sys.platform == 'darwin':
            subprocess.Popen(['open', target])  # noqa: S603,S607
        else:
            subprocess.Popen(['xdg-open', target])  # noqa: S603,S607
        return jsonify({"success": True})
    except Exception as e:
        return jsonify({"success": False, "message": str(e)})


def _normalize_name(value: str) -> str:
    return ''.join(ch.lower() for ch in (value or '') if ch.isalnum())


def _infer_edges_from_columns(table_names, columns_rows, fk_edges, threshold=0.6):
    """
    åŸºäºå‘½åè§„åˆ™æ¨æ–­å…³ç³»ï¼š
    - xxx_id / xxxId / id_xxx ç­‰å¸¸è§æ¨¡å¼
    - è‹¥ç›®æ ‡è¡¨å­˜åœ¨ id åˆ—ï¼Œç½®ä¿¡åº¦æ›´é«˜
    """
    table_set = set(table_names or [])
    norm_to_tables = {}
    for t in table_set:
        norm = _normalize_name(t)
        norm_to_tables.setdefault(norm, []).append(t)
        # ç®€å•å•å¤æ•°å…œåº•ï¼šusers -> user
        if norm.endswith('s'):
            norm_to_tables.setdefault(norm[:-1], []).append(t)

    # build column lookup per table
    table_cols = {}
    for table_name, column_name, data_type in columns_rows or []:
        table_cols.setdefault(table_name, set()).add((column_name or '').lower())

    # build fk set for quick skip
    fk_pairs = set()
    for e in fk_edges or []:
        fk_pairs.add((e.get("from_table"), e.get("to_table")))

    inferred = []
    for table_name, col_name, data_type in columns_rows or []:
        if table_name not in table_set:
            continue
        col = (col_name or '').strip()
        if not col:
            continue
        lower = col.lower()
        if lower == 'id':
            continue

        base = None
        if lower.endswith('_id') and len(lower) > 3:
            base = lower[:-3]
        elif lower.endswith('id') and len(lower) > 2 and lower[-3].isalpha():
            # å¤„ç† userId / userid
            base = lower[:-2]
        elif lower.startswith('id_') and len(lower) > 3:
            base = lower[3:]

        if not base:
            continue

        base_norm = _normalize_name(base)
        if not base_norm:
            continue

        candidates = norm_to_tables.get(base_norm, [])
        # æ’é™¤è‡ªèº«
        candidates = [c for c in candidates if c != table_name]
        # å»é‡ä¿æŒé¡ºåº
        seen = set()
        candidates = [c for c in candidates if not (c in seen or seen.add(c))]

        if len(candidates) != 1:
            continue

        target = candidates[0]
        if (table_name, target) in fk_pairs:
            continue

        confidence = 0.65
        # è‹¥ç›®æ ‡è¡¨å­˜åœ¨ id åˆ—ï¼Œæå‡ç½®ä¿¡åº¦
        if 'id' in table_cols.get(target, set()):
            confidence = 0.8

        if confidence < float(threshold or 0):
            continue

        inferred.append({
            "from_table": table_name,
            "to_table": target,
            "from_column": col_name,
            "to_column": "id" if 'id' in table_cols.get(target, set()) else None,
            "confidence": confidence,
            "reason": f"å­—æ®µå‘½åè§„åˆ™æ¨æ–­ï¼š{col_name} -> {target}",
        })

    return inferred


from .api_graph_mermaid import graph_mermaid


@api_bp.route('/graph', methods=['POST'])
def graph():
    """å…³ç³»å›¾ API - ä½¿ç”¨ Mermaid ER å›¾"""
    return graph_mermaid()
